{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from google.api_core import retry\n",
    "from IPython.display import Markdown\n",
    "\n",
    "load_dotenv()  # API key is stored in .env file\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, a fellow scholar!  Let's delve into the fascinating, and frankly, still somewhat nebulous field of Artificial Intelligence.  Forget the Hollywood sensationalism; we're dealing with a complex interplay of computer science, statistics, and cognitive science.\n",
       "\n",
       "At its core, AI aims to replicate or surpass human cognitive functions in machines.  This encompasses a broad spectrum of approaches,  each with its own theoretical underpinnings and limitations.  We can broadly categorize them as follows:\n",
       "\n",
       "* **Symbolic AI (Good Old-Fashioned AI or GOFAI):** This relies on explicit, rule-based systems.  Think expert systems, where knowledge is encoded as a series of \"if-then\" statements.  While effective for well-defined problems, GOFAI struggles with the inherent ambiguity and complexity of the real world. Its brittle nature – a slight change in input can lead to catastrophic failure – is a significant drawback.  Its legacy, however, is considerable in areas like logic programming and knowledge representation.\n",
       "\n",
       "* **Connectionist AI (Neural Networks):** This paradigm draws inspiration from the structure and function of the human brain.  Artificial neural networks consist of interconnected nodes (neurons) that process information in parallel.  Through training on large datasets, these networks learn complex patterns and relationships, exhibiting impressive performance in areas like image recognition, natural language processing, and game playing.  However, their \"black box\" nature, the difficulty in interpreting their decision-making processes, remains a significant challenge, especially regarding explainability and trustworthiness.  Deep learning, a subfield utilizing multiple layers of interconnected nodes, has revolutionized many aspects of AI.\n",
       "\n",
       "* **Evolutionary AI:** This approach leverages principles of biological evolution – selection, mutation, and reproduction – to optimize AI systems. Genetic algorithms and other evolutionary computation techniques are used to explore vast solution spaces and discover optimal solutions for complex problems, often outperforming traditional optimization methods.\n",
       "\n",
       "* **Hybrid Approaches:** Many successful AI systems blend elements of these paradigms.  For example, a robotic system might employ neural networks for perception, symbolic AI for planning, and evolutionary algorithms for adaptation.  The synergy created by integrating various approaches is often crucial for achieving robust and adaptable AI.\n",
       "\n",
       "Beyond these core methodologies, important considerations include:\n",
       "\n",
       "* **Machine Learning (ML):** A subset of AI focusing on algorithms that allow systems to learn from data without explicit programming.  Supervised, unsupervised, and reinforcement learning are prominent subfields.\n",
       "\n",
       "* **Deep Learning (DL):** A subset of ML using artificial neural networks with multiple layers.\n",
       "\n",
       "* **Explainable AI (XAI):** A crucial area of research focusing on developing AI systems whose decisions are transparent and understandable to humans. This addresses concerns about bias, fairness, and accountability.\n",
       "\n",
       "* **Ethical Considerations:** The societal impact of AI necessitates a rigorous examination of its ethical implications, including bias, privacy, job displacement, and the potential for misuse.\n",
       "\n",
       "In conclusion, AI is not a monolithic entity but a vibrant and evolving field encompassing numerous approaches, each with its strengths and limitations.  Its ongoing development presents both incredible opportunities and significant challenges, demanding critical analysis and responsible innovation from the academic community.  Further research is vital across the spectrum, from theoretical foundations to practical applications, ensuring the responsible and beneficial deployment of this powerful technology.  What specific areas of AI research are you currently pursuing, Professor?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a professor\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = flash.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down how a decoder-only transformer like GPT processes a prompt and generates a response step-by-step, focusing on the underlying mechanisms.  We'll simplify some aspects for clarity, but the core concepts remain accurate.\n",
       "\n",
       "**1. Tokenization:**\n",
       "\n",
       "* **Input:** The process begins with the user's prompt.  GPT doesn't understand raw text; it needs to be converted into numerical tokens.  This is done using a tokenizer, a vocabulary that maps words (or sub-word units) to unique integer IDs.  For example, \"Hello, how are you?\" might become a sequence of token IDs like [12, 3, 45, 6, 78, 9].  Special tokens like `<BOS>` (beginning of sequence) and `<EOS>` (end of sequence) are often added.\n",
       "\n",
       "**2. Embedding:**\n",
       "\n",
       "* **Input:** The sequence of token IDs.\n",
       "* **Process:** Each token ID is converted into a dense vector representation called an embedding.  These embeddings capture semantic information about the tokens.  \"Hello\" and \"Hi\" might have similar embeddings because they convey similar greetings.  This embedding process happens through a learned embedding matrix; each row represents a token's embedding.\n",
       "\n",
       "**3. Positional Encoding:**\n",
       "\n",
       "* **Input:** The sequence of embeddings.\n",
       "* **Process:**  Unlike encoder-decoder transformers, decoder-only models don't inherently know the order of words.  Positional encodings are added to the embeddings to provide information about the position of each token in the sequence. This is crucial for understanding sentence structure and context.  Different methods exist for positional encoding, including sinusoidal functions or learned embeddings.\n",
       "\n",
       "**4. Decoder Layers (Iteration):** This is the heart of the process, and it happens iteratively, one token at a time.\n",
       "\n",
       "* **Input:** For the *first* iteration, the input is the initial sequence of embedded tokens with positional encodings (the prompt).  For subsequent iterations, the input includes the previously generated tokens.\n",
       "\n",
       "* **Self-Attention Mechanism:**  The decoder's core is the self-attention mechanism.  It allows the model to weigh the importance of different tokens in the input sequence when generating the next token.  For example, when generating the next word after \"Hello, how,\" the model might pay more attention to \"how\" than \"Hello\" because \"how\" is more closely related to the expected next word. This is done through three matrices (Query, Key, Value) derived from the input embeddings and calculated using dot product attention.  The softmax function ensures that the attention weights sum up to 1.\n",
       "\n",
       "* **Multi-Head Attention:** Instead of just one self-attention mechanism, GPT uses multiple \"heads,\" each focusing on different aspects of the input sequence. The outputs of these heads are concatenated and linearly transformed.\n",
       "\n",
       "* **Add & Norm:** The output of the multi-head attention is added to the input embeddings (residual connection) and then layer normalized (to stabilize training).\n",
       "\n",
       "* **Feed-Forward Network:** A fully connected feed-forward network is applied to the output of the add & norm layer.  This further processes the information.  Another add & norm layer follows.\n",
       "\n",
       "* **Output:** After the decoder layer, we have a refined representation of the input sequence, informed by self-attention.\n",
       "\n",
       "**5. Output Layer (and Iteration):**\n",
       "\n",
       "* **Input:** The output of the final decoder layer.\n",
       "* **Process:**  This output is passed through a linear layer and a softmax function. The softmax function produces a probability distribution over the entire vocabulary. The token with the highest probability is selected as the next token in the generated sequence.\n",
       "* **Iteration:** This process repeats. The newly generated token is added to the input sequence, and the decoder layers process this extended sequence to generate the next token.  This continues until a special `<EOS>` token is generated or a maximum length is reached.\n",
       "\n",
       "**6. Detokenization:**\n",
       "\n",
       "* **Input:** The sequence of generated token IDs.\n",
       "* **Process:** Finally, the sequence of token IDs is converted back into human-readable text using the reverse mapping of the tokenizer.\n",
       "\n",
       "**In Summary:** GPT processes a prompt by iteratively refining its understanding of the context using self-attention, generating one token at a time based on the probability distribution learned from massive amounts of text data.  The self-attention mechanism is key to its ability to capture long-range dependencies and generate coherent and contextually relevant text.  The iterative nature allows for a dynamic generation process that adapts to the evolving sequence of words.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"Hello can you explain how decoder only transformers like GPT processes a prompt and generates a response step by step in reference to underlying mechanism in detalit\"\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine a team of gossipy parrots.  They've overheard snippets of a conversation – a fragmented, jumbled mess of words.  Each parrot only hears a *tiny* piece, but they're all connected via a complex, extremely noisy, system of squawks and wing-flaps (the attention mechanism).\n",
       "\n",
       "The goal?  To reconstruct the *entire* conversation, despite only ever hearing fragments and constantly being distracted by the other parrots' chaotic chatter.  They don't have a transcript to refer to; they only have their own messy, unreliable memories of bits and pieces.  They squawk at each other, judging the validity of each other's interpretations (self-attention), adjusting their own understanding based on the other parrots' \"input\" (cross-attention), and gradually, through sheer force of squawking and chaotic cooperation, manage to build a coherent – though perhaps slightly inaccurate and highly embellished – version of the original conversation.  That reconstructed conversation is the output of the decoder-only transformer.\n",
       "\n",
       "The parrots represent the layers, the squawks are the self-attention, and the wildly inaccurate but kinda-sorta-coherent final version is the generated text. The original conversation is the prompt, and the inaccuracies are caused by overfitting, attention dropout and all manner of feathered fiascoes.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = genai.GenerationConfig(\n",
    "    max_output_tokens=1000,\n",
    "    temperature=1.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(\n",
    "        predicate=retry.if_transient_error,\n",
    "        initial=5,\n",
    "        multiplier=1.5,\n",
    "        timeout=300,\n",
    "    )\n",
    "}\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=model_config,\n",
    ")\n",
    "\n",
    "response = high_temp_model.generate_content(\n",
    "    \"Come up with the most ridiculous analogy for explaining decoder only transformers model\",\n",
    "    request_options=retry_policy,\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "TRANSFORMER"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enum output\n",
    "import enum\n",
    "\n",
    "\n",
    "class Seq2SeqModel(enum.Enum):\n",
    "    LSTM = \"LSTM\"\n",
    "    GRU = \"GRU\"\n",
    "    TRANSFORMER = \"TRANSFORMER\"\n",
    "\n",
    "\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(\n",
    "        predicate=retry.if_transient_error,\n",
    "        initial=5,\n",
    "        multiplier=1.5,\n",
    "        timeout=300,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "model_config = genai.GenerationConfig(\n",
    "    max_output_tokens=100,\n",
    "    temperature=0.2,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    response_mime_type=\"text/x.enum\",\n",
    "    response_schema=Seq2SeqModel,\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    \"gemini-1.5-flash-001\",\n",
    "    generation_config=model_config,\n",
    ")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"What is the best type of machine learning model for language translation?\",\n",
    "    request_options=retry_policy,\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"context_size\": 1012, \"pretraining_datasets\": [\"WebText\"]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JSON output\n",
    "import typing_extensions as typing\n",
    "\n",
    "\n",
    "class TransformerModel(typing.TypedDict):\n",
    "    context_size: int\n",
    "    pretraining_datasets: list[str]\n",
    "    name: str\n",
    "\n",
    "\n",
    "model_config = genai.GenerationConfig(\n",
    "    max_output_tokens=100,\n",
    "    temperature=0.2,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=TransformerModel,\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\", generation_config=model_config)\n",
    "\n",
    "prompt_text = \"\"\"Parse a given transformer specs into valid JSON:\n",
    "\n",
    "EXAMPLE: BERT model has context_size of 512 and datasets as BookCorpus, English Wikipedia\n",
    "\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"context_size\": 512,\n",
    "\"pretraining_dataset\": [\"BookCorpus\", \"English Wikipedia\"],\n",
    "\"name\": \"BERT\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "question_text = \"GPT-2 has context_size of 1012 and datasets as WebText\"\n",
    "\n",
    "response = model.generate_content([prompt_text, question_text])\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Step 1: Find the partner's age when you were 4.\n",
       "\n",
       "* You were 4, and your partner was three times your age, so your partner was 4 * 3 = 12 years old.\n",
       "\n",
       "Step 2: Find the age difference between you and your partner.\n",
       "\n",
       "* The age difference is 12 - 4 = 8 years.\n",
       "\n",
       "Step 3: Calculate your partner's current age.\n",
       "\n",
       "* You are now 20 years old.\n",
       "* Your partner is 8 years older than you, so your partner is 20 + 8 = 28 years old.\n",
       "\n",
       "Therefore, your partner is now $\\boxed{28}$ years old.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain of thought\n",
    "model_config = genai.GenerationConfig(\n",
    "    max_output_tokens=1000,\n",
    "    temperature=1.5,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(\n",
    "        predicate=retry.if_transient_error,\n",
    "        initial=5,\n",
    "        multiplier=1.5,\n",
    "        timeout=300,\n",
    "    )\n",
    "}\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=model_config,\n",
    ")\n",
    "\n",
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = high_temp_model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
